import streamlit as st
from pathlib import Path
import sys
import os

# --- ê²½ë¡œ ì„¤ì • ---
BASE_DIR = Path(__file__).resolve().parent.parent
MODULE_PATH = BASE_DIR / "back" / "retriever" / "pdf"
sys.path.append(str(MODULE_PATH))

# --- modules.pyì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸° ---
from modules import load_embedding, load_db, load_llm_chain

# --- ì´ˆê¸°í™” ---
embedding = load_embedding()
db = load_db(embedding)
llm_chain = load_llm_chain()

# --- Streamlit UI ---
st.set_page_config(page_title="í˜„ëŒ€ì°¨ ë§¤ë‰´ì–¼ QA", layout="wide")
st.title("ğŸš— í˜„ëŒ€ìë™ì°¨ ë§¤ë‰´ì–¼ ê¸°ë°˜ RAG QA ì‹œìŠ¤í…œ")

# ë“±ë¡ëª¨ë¸ ëª¨ë¸ ì•ˆë‚´
known_models = ["ì•„ë°˜ë–¼", "ì‹¼íƒ€í˜", "íˆ¬ì‹¼", "ìºìŠ¤í¼", "ìŠ¤íƒ€ë¦¬ì•„", "ê·¸ëœì €", "ì†Œë‚˜íƒ€"]
st.warning(f"í˜„ì¬ ì§€ì›í•˜ëŠ” ì°¨ëŸ‰ ëª¨ë¸ì€ {', '.join(known_models)}ì…ë‹ˆë‹¤. ì§ˆë¬¸ì‹œ ì°¨ëŸ‰ì„ í•¨ê»˜ ì§ˆë¬¸í•´ì£¼ì„¸ìš”")

# ì„¸ì…˜ ìƒíƒœì— ì§ˆë¬¸ íˆìŠ¤í† ë¦¬ ì €ì¥
if "history" not in st.session_state:
    st.session_state.history = []    
    
query = st.text_input("â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì‹¼íƒ€í˜ì˜ ë’·ìœ ë¦¬ ì™€ì´í¼ê°€ ì‘ë™í•˜ì§€ ì•Šì„ ë•Œ í™•ì¸í•  ì‚¬í•­ì€)")


if query:
    matched_model = next((m for m in known_models if m in query), "ë¯¸ì§€ì •")
    # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
    relevant_docs = db.similarity_search(query, k=10)
    
    # ëª¨ë¸ ì¼ì¹˜ ìš°ì„  í•„í„°ë§
    filtered_docs = [doc for doc in relevant_docs if doc.metadata.get("model") == matched_model]
    selected_docs = filtered_docs[:3] if filtered_docs else relevant_docs[:3]

    # ì°¸ê³  ë¬¸ì„œ ì—†ì„ ê²½ìš° ë©”ì‹œì§€ ì¶œë ¥
    if not selected_docs:
        st.error("âŒ ê´€ë ¨ ì°¨ëŸ‰ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ ë‚´ìš©ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")
    else:    
        # ìƒì„±ì„±
        context_blocks = []
        model_count = {}
        for i, doc in enumerate(selected_docs):
            model = doc.metadata.get("model", "ë¯¸ì§€ì •")
            pages = doc.metadata.get("pages", ["?"])
            model_count[model] = model_count.get(model, 0) + 1
            citation = f"[page {pages[0]} / model: {model}]"
            context_blocks.append(f"{citation}\n{doc.page_content}")
        context = "\n\n".join(context_blocks)

        # ê°€ì¥ ë§ì€ ëª¨ë¸ ê¸°ì¤€
        if model_count:
            most_common_model = max(model_count, key=model_count.get)
        else:
            most_common_model = "ë¯¸ì§€ì •"

        
        # qa
        result = llm_chain.invoke({
            "query": query,
            "context": context,
            "model": most_common_model
        })

        # ë‹µë³€ë§Œ ì¶”ì¶œ
        full_output = result["text"]
        answer = full_output.split("ë‹µë³€:")[-1].strip() if "ë‹µë³€:" in full_output else full_output.strip()

        # íˆìŠ¤í† ë¦¬ì— ì €ì¥
        st.session_state.history.append((query, answer))

        # ë‹µë³€ ì¶œë ¥
        st.markdown("### ğŸ’¬ ë‹µë³€")
        st.write(answer)

        st.markdown("### ğŸ“„ ì°¸ê³  ë¬¸ì„œ")
        for i, doc in enumerate(selected_docs, 1):
            model = doc.metadata.get("model", "")
            pages = doc.metadata.get("pages", ["?"])
            st.markdown(f"**[{i}] page {pages[0]}, model: {model}**")
            st.write(doc.page_content[:500] + ("..." if len(doc.page_content) > 500 else ""))

#  ì´ì „ ì§ˆë¬¸ íˆìŠ¤í† ë¦¬
if st.session_state.history:
    st.markdown("### ğŸ“š ì´ì „ ì§ˆë¬¸")
    for q, a in reversed(st.session_state.history[-5:]):  # ìµœê·¼ 5ê°œë§Œ
        st.markdown(f"**â“ {q}**")
        st.markdown(f"**ğŸ’¬ {a}**")
        st.markdown("---")