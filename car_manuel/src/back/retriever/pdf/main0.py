# src/back/retriever/pdf/main.py

import os
from pathlib import Path
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.llms import HuggingFacePipeline
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

# === ì„¤ì • ===
index_path = './data/parsed/pdfminer/embedding/faiss_index'
embedding_model_name = "BAAI/bge-m3"
llm_model_name = "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"

# === í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ ===
template = """
ë„ˆëŠ” í˜„ëŒ€ìë™ì°¨ ë§¤ë‰´ì–¼ ê¸°ë°˜ AIì•¼.
ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì¤˜.
ê° ë¬¸ì„œëŠ” íŠ¹ì • ì°¨ëŸ‰ ëª¨ë¸({model})ì— ëŒ€í•œ ë‚´ìš©ì´ì•¼.
í˜„ì¬ ë¬¸ì„œì—ëŠ” ì•„ë°˜ë–¼, ì‹¼íƒ€í˜, íˆ¬ì‹¼, ìŠ¤íƒ€ë¦¬ì•„, ì¼€ìŠ¤í¼, ê·¸ëœì €, ì†Œë‚˜íƒ€ì— ëŒ€í•œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ìˆì–´
ê´€ë ¨ ë‚´ìš© ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•´ì„œ ë°”íƒ•ìœ¼ë¡œ ì•ˆë‚´í•´ì¤˜.
ì°¸ê³ í•œ ragë¬¸ì„œì˜ í˜ì´ì§€ì™€ ì°¨ëŸ‰ ì´ë¦„ì„ í•­ìƒ ê°™ì´ ì¶œë ¥í•´ì¤˜.
ragë¬¸ì„œ ë‚´ìš©ì„ ëª…í™•í•˜ê²Œ íŒŒì•…í•˜ê³  ë‹µë³€í•´ì¤˜.
ë¬¸ì„œ ë‚´ìš©ê³¼ í•¨ê»˜ [ì¶œì²˜ ì •ë³´]ë„ í¬í•¨ë˜ì–´ ìˆì–´. ì˜ˆì‹œì²˜ëŸ¼ í˜ì´ì§€ ë²ˆí˜¸ì™€ ì°¨ëŸ‰ ëª¨ë¸ì„ ì¸ìš©í•´ì¤˜:
ì˜ˆ) [page ë²ˆí˜¸, model: ì°¨ì¢…]
ì§ˆë¬¸í•œ ì°¨ëŸ‰ ëª¨ë¸ê³¼ ë¬¸ì„œì˜ ëª¨ë¸ëª…ì´ ì¼ì¹˜í•  ê²½ìš° í•´ë‹¹ ë‚´ìš©ì„ ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ì •ë³´ë¡œ ê°„ì£¼í•´ì„œ ë‹µë³€ì„ êµ¬ì„±í•´ì¤˜.
ë‹¤ë¥¸ ì°¨ëŸ‰ ë¬¸ì„œì— ìœ ì‚¬í•œ ì •ë³´ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ í•„ìš”ì‹œ ì°¸ê³ í•˜ê³  ì–´ë–¤ ë¬¸ì„œë„ í•¨ê»˜ ì°¸ê³ í–ˆëŠ”ì§€ ëª…ì‹œí•´ì¤˜
ë‹¤ë¥¸ ì°¨ëŸ‰ ë¬¸ì„œë¥¼ ì°¸ê³ í•œ ê²½ìš°, ì–´ë–¤ ë¬¸ì„œë¥¼ ì°¸ê³ í–ˆëŠ”ì§€ë„ ëª…í™•íˆ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì¤˜. ê·¸ëŸ¬ë‚˜ ìš°ì„ ì ìœ¼ë¡œ ì§ˆë¬¸ê³¼ ê°™ì€ ì°¨ëŸ‰ì˜ ë¬¸ì„œë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•´ì„œ ë‹µë³€í•´
ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ì¡°ê±´ì´ë‚˜ ìƒí™©ì´ ëª…ì‹œë˜ì§€ ì•Šì€ ê²½ìš°, ë‹µë³€ì„ ìœ ë³´í•˜ê±°ë‚˜ ì¶”ë¡ ëœ ì„¤ëª…ì€ "ì¶”ì •"ì„ì„ ë¶„ëª…íˆ í•´ì¤˜.
ì°¸ê³  í•  ìˆ˜ ìˆëŠ” ë¬¸ì„œê°€ ì—†ì—ˆë‹¤ë©´ "í•´ë‹¹ ì°¨ëŸ‰ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  í•„ìˆ˜ì ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸:
{question}

ë‹µë³€ì„ ë¬¸ì¥ìœ¼ë¡œ ì™„ì„±í•´ì¤˜.
ë‹µë³€:
"""

custom_prompt = PromptTemplate(
    input_variables=["context", "query", "model"],
    template=template.replace("{question}", "{query}")
)


# === ëª¨ë“ˆí™”ëœ í•¨ìˆ˜ë“¤ ===

def load_embedding():
    return HuggingFaceBgeEmbeddings(
        model_name=embedding_model_name,
        model_kwargs={"device": "cuda:0"},
        encode_kwargs={"normalize_embeddings": True},
    )

def load_db(embedding):
    return FAISS.load_local(index_path, embedding, allow_dangerous_deserialization=True)

def load_llm_chain():
    tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
    model = AutoModelForCausalLM.from_pretrained(
        llm_model_name,
        trust_remote_code=True
    ).to("cuda")

    llm_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device=0,
        max_new_tokens=512,
        do_sample=False,
    )

    llm = HuggingFacePipeline(pipeline=llm_pipeline)
    return LLMChain(llm=llm, prompt=custom_prompt, output_key="text")


# === ì™¸ë¶€ì—ì„œ import ê°€ëŠ¥í•œ ê°ì²´ ===
embedding = load_embedding()
db = load_db(embedding)
llm_chain = load_llm_chain()


# === CLI ì¸í„°í˜ì´ìŠ¤ ===
def run_cli():
    print("ğŸš— ìë™ì°¨ ë§¤ë‰´ì–¼ ê¸°ë°˜ RAG QA ì‹œìŠ¤í…œ ì…ë‹ˆë‹¤. ì•Œê³ ì í•˜ëŠ” ì°¨ì¢…ì„ í•¨ê»˜ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    print("â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” ('exit' ì…ë ¥ ì‹œ ì¢…ë£Œ ë©ë‹ˆë‹¤.)\n")

    known_models = ["ì•„ë°˜ë–¼", "ì‹¼íƒ€í˜", "íˆ¬ì‹¼", "ìºìŠ¤í¼", "ìŠ¤íƒ€ë¦¬ì•„", "ê·¸ëœì €", "ì†Œë‚˜íƒ€"]

    while True:
        query = input("ì§ˆë¬¸: ").strip()
        if query.lower() in ["exit", "quit"]:
            print("ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.")
            break

        matched_model = next((m for m in known_models if m in query), "ë¯¸ì§€ì •")

        relevant_docs = db.similarity_search(query, k=10)
        filtered_docs = [doc for doc in relevant_docs if doc.metadata.get("model") == matched_model]
        selected_docs = filtered_docs[:3] if filtered_docs else relevant_docs[:3]

        model_count = {}
        for doc in relevant_docs:
            model = doc.metadata.get("model", "ë¯¸ì§€ì •")
            model_count[model] = model_count.get(model, 0) + 1
        most_common_model = max(model_count, key=model_count.get)

        context_blocks = []
        for i, doc in enumerate(selected_docs, 1):
            pages = doc.metadata.get("pages", [])
            model = doc.metadata.get("model", "")
            citation = f"[page {pages[0] if pages else '?'} / model: {model}]"
            context_blocks.append(f"{citation}\n{doc.page_content}")
        context = "\n\n".join(context_blocks)

        result = llm_chain.invoke({
            "query": query,
            "context": context,
            "model": most_common_model
        })

        full_output = result["text"]
        if "ë‹µë³€:" in full_output:
            answer = full_output.split("ë‹µë³€:")[-1].strip()
        else:
            answer = full_output.strip()

        print("\nğŸ’¬ ë‹µë³€:")
        print(answer)


# === ì‹¤í–‰ ì¡°ê±´ ===
if __name__ == "__main__":
    run_cli()
