#rag pipline -> langchain + faiss 

import os
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
# from langchain_huggingface import HuggingFaceBgeEmbeddings  ! bgeì™€ í˜¸í™˜ ì—ëŸ¬ì¸ë“¯ í•¨

from langchain_community.llms import HuggingFacePipeline
from langchain.chains import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

index_path = './data/parsed/pdfminer/embedding/faiss_index'
embedding_model_name = "BAAI/bge-m3"
llm_model_name = "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"

#ëª¨ë¸ ë¡œë“œ
embedding = HuggingFaceBgeEmbeddings(
    model_name=embedding_model_name,
    model_kwargs={"device": "cuda:0"},
    encode_kwargs={"normalize_embeddings": True},
)

# ë²¡í„° db ë¡œë“œ
db = FAISS.load_local(index_path, embedding, allow_dangerous_deserialization=True)

# === [4] LLM ë¡œë”© (EXAONE) ===
tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
model = AutoModelForCausalLM.from_pretrained(
    llm_model_name,
    trust_remote_code=True
).to("cuda")

llm_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=0,
    max_new_tokens=512,
    do_sample=False,
)

llm = HuggingFacePipeline(pipeline=llm_pipeline)

#í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
template = """
ë„ˆëŠ” í˜„ëŒ€ìë™ì°¨ ë§¤ë‰´ì–¼ ê¸°ë°˜ AIì•¼.
ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì¤˜.
ê° ë¬¸ì„œëŠ” íŠ¹ì • ì°¨ëŸ‰ ëª¨ë¸({model})ì— ëŒ€í•œ ë‚´ìš©ì´ì•¼.
í˜„ì¬ ë¬¸ì„œì—ëŠ” ì•„ë°˜ë–¼, ì‹¼íƒ€í˜, ì•„ì´ì˜¤ë‹‰5ì— ëŒ€í•œ ì •ë³´ê°€ í¬í•¨ë˜ì–´ìˆì–´
ê´€ë ¨ ë‚´ìš© ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•´ì„œ ë°”íƒ•ìœ¼ë¡œ ì•ˆë‚´í•´ì¤˜.
ì°¸ê³ í•œ ragë¬¸ì„œì˜ í˜ì´ì§€ì™€ ì°¨ëŸ‰ ì´ë¦„ì„ í•­ìƒ ê°™ì´ ì¶œë ¥í•´ì¤˜.
ragë¬¸ì„œ ë‚´ìš©ì„ ëª…í™•í•˜ê²Œ íŒŒì•…í•˜ê³  ë‹µë³€í•´ì¤˜.
ë¬¸ì„œ ë‚´ìš©ê³¼ í•¨ê»˜ [ì¶œì²˜ ì •ë³´]ë„ í¬í•¨ë˜ì–´ ìˆì–´. ì˜ˆì‹œì²˜ëŸ¼ í˜ì´ì§€ ë²ˆí˜¸ì™€ ì°¨ëŸ‰ ëª¨ë¸ì„ ì¸ìš©í•´ì¤˜:
ì˜ˆ) [page 5, model: ì•„ë°˜ë–¼]
ì§ˆë¬¸í•œ ì°¨ëŸ‰ ëª¨ë¸ê³¼ ë¬¸ì„œì˜ ëª¨ë¸ëª…ì´ ì¼ì¹˜í•  ê²½ìš° í•´ë‹¹ ë‚´ìš©ì„ ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ì •ë³´ë¡œ ê°„ì£¼í•´ì„œ ë‹µë³€ì„ êµ¬ì„±í•´ì¤˜.
ë‹¤ë¥¸ ì°¨ëŸ‰ ë¬¸ì„œì— ìœ ì‚¬í•œ ì •ë³´ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ í•„ìš”ì‹œ ì°¸ê³ í•˜ê³  ì–´ë–¤ ë¬¸ì„œë„ í•¨ê»˜ ì°¸ê³ í–ˆëŠ”ì§€ ëª…ì‹œí•´ì¤˜

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸:
{question}

ë‹µë³€ì„ ë¬¸ì¥ìœ¼ë¡œ ì™„ì„±í•´ì¤˜.
ë‹µë³€:
"""

custom_prompt = PromptTemplate(
    input_variables=["context", "query", "model"],
    template=template.replace("{question}", "{query}")
)

# langchain rag qa ìƒì„±
llm_chain = LLMChain(llm=llm, prompt=custom_prompt)

# ì§ˆì˜ ë£¨í”„ 
print("ğŸš— ìë™ì°¨ ë§¤ë‰´ì–¼ ê¸°ë°˜ RAG QA ì‹œìŠ¤í…œ ì…ë‹ˆë‹¤. ì•Œê³ ì í•˜ëŠ” ì°¨ì¢…ì„ í•¨ê»˜ ì…ë ¥í•´ì£¼ì„¸ìš”.")
print("â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” ('exit' ì…ë ¥ ì‹œ ì¢…ë£Œ ë©ë‹ˆë‹¤.)\n")

while True:
    query = input("ì§ˆë¬¸: ").strip()
    if query.lower() in ["exit", "quit"]:
        break
    
    # ì§ˆë¬¸ì—ì„œ ì°¨ì¢… ì¶”ì¶œ 
    known_models = ["ì•„ë°˜ë–¼", "ì‹¼íƒ€í˜", "ì•„ì´ì˜¤ë‹‰5"]
    matched_model = next((m for m in known_models if m in query), "ë¯¸ì§€ì •")
    
    # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
    relevant_docs = db.similarity_search(query, k=3)
    
     # ì°¨ì¢…ì¢… ì¼ì¹˜ ìš°ì„  í•„í„°ë§
    filtered_docs = [doc for doc in relevant_docs if doc.metadata.get("model") == matched_model]
    selected_docs = filtered_docs[:3] if filtered_docs else relevant_docs[:3]
    
    # ì°¨ì¢…(model) ì •ë³´ ì¶”ì¶œ
    model_count = {}
    for doc in relevant_docs:
        model = doc.metadata.get("model", "ë¯¸ì§€ì •")
        model_count[model] = model_count.get(model, 0) + 1
    most_common_model = max(model_count, key=model_count.get)

    # context êµ¬ì„±(ì¶œì²˜í¬í•¨)
    context_blocks = []
    for doc in selected_docs:
        pages = doc.metadata.get("pages", [])
        model = doc.metadata.get("model", "")
        citation = f"[page {pages[0] if pages else '?'} / model: {model}]"
        context_blocks.append(f"{citation}\n{doc.page_content}")
    context = "\n\n".join(context_blocks)

    # QA ì‹¤í–‰
    result = llm_chain.invoke({
        "query": query,
        "context": context,
        "model": most_common_model
    })

    # ì¶œë ¥
    print("\nğŸ’¬ ë‹µë³€:")
    print(result["text"])

    print("\nğŸ“„ ì°¸ê³  ë¬¸ì„œ:")
    for i, doc in enumerate(relevant_docs, 1):
        print(f"[{i}] {doc.metadata.get('type', '')} / pages: {doc.metadata.get('pages', [])} / model: {doc.metadata.get('model', '')}")
        print(doc.page_content[:300])
        print("-" * 40)
