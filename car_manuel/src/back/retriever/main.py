import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.llms import HuggingFacePipeline
from langchain_community.vectorstores import FAISS 
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate
from langchain_huggingface import HuggingFacePipeline


# ê²½ë¡œ ì„¤ì •
index_path = "data/embeddings/faiss_index"
embed_model_name = "BAAI/bge-m3"
llm_model_name = "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct"

# ì„ë² ë”© ëª¨ë¸
embedding_model = HuggingFaceEmbeddings(
    model_name=embed_model_name,
    model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"}
)

# ë²¡í„° db ë¶ˆëŸ¬ì˜¤ê¸°
db = FAISS.load_local(index_path, embedding_model, allow_dangerous_deserialization=True)

# retriver
retriever = db.as_retriever(search_kwargs={"k": 3})

# ëª¨ë¸ ë¡œë”©
model = AutoModelForCausalLM.from_pretrained(
    llm_model_name,
    trust_remote_code=True
).to("cuda")

tokenizer = AutoTokenizer.from_pretrained(
    llm_model_name,
    trust_remote_code=True
)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=0,
    max_new_tokens=512,
)

llm = HuggingFacePipeline(pipeline=pipe)

# #í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿(ëª…ì‹œ)
# template = """ë„ˆëŠ” í˜„ëŒ€, ê¸°ì•„ ìë™ì°¨ ë©”ë‰´ì–¼ ê¸°ë°˜ aiì•¼ ë¬¸ì„œë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬
# ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜, ë˜í•œ ì–´ë–¤ ì°¨ì¢…ì¸ì§€ ì–˜ê¸°í•´ì¤˜.

# ë¬¸ì„œ ë‚´ìš©:
# {context}

# ì§ˆë¬¸: {question}
# ë‹µë³€:
# """
# prompt = PromptTemplate.from_template(template)

#qa_chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt},
    input_key = "question"    
)

# ì‹¤í–‰
if __name__ == "__main__":
    while True:
        query = input("â“ í˜„ëŒ€, ê¸°ì•„ ìë™ì°¨ ë©”ë‰´ì–¼ ê¸°ë°˜ ì±—ë´‡ì…ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ì°¨ì¢…ì„ ì…ë ¥í•˜ì„¸ìš” (exit ì…ë ¥ ì‹œ ì¢…ë£Œ): ")
        if query.lower() == "exit":
            print('ê°ì‚¬í•©ë‹ˆë‹¤. ë‹¤ìŒì— ë˜ ì‚¬ìš©í•´ì£¼ì„¸ìš”')
            break
        print(f"ğŸ‘‰ ì…ë ¥ëœ ì§ˆë¬¸: {query}")
        docs = retriever.get_relevant_documents(query)
        print("\nğŸ“„ ì°¸ê³ í•œ ë¬¸ì„œ:\n")
        for i, doc in enumerate(docs):
            print(f"[{i+1}] {doc.metadata.get('source', 'ë¬¸ì„œ')}: {doc.page_content[:100]}...\n")
        result = qa_chain.invoke({"question": query})
        print(f"\nğŸ’¬ ë‹µë³€: {result['result']}\n")