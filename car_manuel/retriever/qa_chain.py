import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.llms import HuggingFacePipeline
from langchain_community.vectorstores import FAISS 
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate

# ê²½ë¡œ ì„¤ì •
index_path = "data/embeddings/faiss_index"
embed_model_name = "BAAI/bge-m3"
llm_model_name = "nlpai-lab/kullm3"

# ì„ë² ë”© ëª¨ë¸
embedding_model = HuggingFaceEmbeddings(
    model_name=embed_model_name,
    model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"}
)

# ë²¡í„° db ë¶ˆëŸ¬ì˜¤ê¸°
db = FAISS.load_local(index_path, embedding_model, allow_dangerous_deserialization=True)

# retriver
retriever = db.as_retriever(search_kwargs={"k": 3})

# ëª¨ë¸ ë¡œë”©
tokenizer = AutoTokenizer.from_pretrained(llm_model_name)
model = AutoModelForCausalLM.from_pretrained(llm_model_name).to("cuda")

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=0,
    max_new_tokens=512,
)

llm = HuggingFacePipeline(pipeline=pipe)

#í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿(ëª…ì‹œ)
template = """ë„ˆëŠ” í˜„ëŒ€, ê¸°ì•„ ìë™ì°¨ ë©”ë‰´ì–¼ ê¸°ë°˜ aiì•¼ ë¬¸ì„œë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬
ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜, ë˜í•œ ì–´ë–¤ ì°¨ì¢…ì¸ì§€ ì–˜ê¸°í•´ì¤˜
ì§ˆë¬¸: {question}
ë‹µë³€:
"""
prompt = PromptTemplate.from_template(template)

#qa_chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt}
)

# ì‹¤í–‰
if __name__ == "__main__":
    while True:
        query = input("â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (exit ì…ë ¥ ì‹œ ì¢…ë£Œ): ")
        if query.lower() == "exit":
            break
        answer = qa_chain.run(query)
        print(f"\nğŸ’¬ ë‹µë³€: {answer}\n")